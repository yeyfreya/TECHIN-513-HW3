{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLRJNDGiPU37"
      },
      "source": [
        "## TECHIN 513 - Basic ML\n",
        "\n",
        "##### Freya Yu - 2372732 - yeyfreya@uw.edu\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "Install the required packages (scikit-learn, TensorFlow, Keras, PyTorch, and, pandas) if they are not already installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JVHNfDPhPjyw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Classifier Accuracy: 1.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 1s 583us/step - loss: 0.2582 - accuracy: 0.9257\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 1s 579us/step - loss: 0.1114 - accuracy: 0.9669\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 1s 610us/step - loss: 0.0752 - accuracy: 0.9775\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 1s 586us/step - loss: 0.0571 - accuracy: 0.9825\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 1s 603us/step - loss: 0.0437 - accuracy: 0.9862\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 1s 597us/step - loss: 0.0341 - accuracy: 0.9893\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 2s 874us/step - loss: 0.0279 - accuracy: 0.9910\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 1s 592us/step - loss: 0.0223 - accuracy: 0.9929\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 1s 567us/step - loss: 0.0173 - accuracy: 0.9946\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 2s 884us/step - loss: 0.0146 - accuracy: 0.9955\n",
            "313/313 [==============================] - 0s 412us/step - loss: 0.0860 - accuracy: 0.9768\n",
            "Test Loss: 0.0859656035900116, Test Accuracy: 0.9768000245094299\n",
            "Weight: 1.9921954870224, Bias: 0.017703142017126083\n"
          ]
        }
      ],
      "source": [
        "# use pip to install the packages\n",
        "# !pip install scikit-learn TensorFlow Keras PyTorch pandas numpy\n",
        "\n",
        "# Import necessary packages\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import RandomForestClassifier from sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Task 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Task 2: Split the data into training and testing sets\n",
        "# use train_test_split function to split the data with test_size = 0.2 and random_state = 42\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Task 3: Train a Random Forest Classifier on the training data\n",
        "# import RandomForestClassifier from sklearn and fit it with training data\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Task 4: Evaluate the classifier on the testing data\n",
        "# use clf.score function to evaluate the classifier on the testing data\n",
        "# print the accuracy of the classifier\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy}\")\n",
        "\n",
        "# Task 5: Load the MNIST dataset\n",
        "# use keras.datasets.mnist.load_data() to load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Task 6: Preprocess the data\n",
        "# normalize the data by dividing by 255.0\n",
        "# use to_categorical from keras.utils to one-hot encode the labels\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Task 7: Define and train a simple neural network using Keras\n",
        "# use Sequential model from keras.models\n",
        "# use Dense layer from keras.layers\n",
        "# use 'adam' as optimizer and 'categorical_crossentropy' as loss function\n",
        "# use model.fit to train the model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(784,)),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train.reshape(-1, 784), y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Task 8: Evaluate the neural network on the testing data\n",
        "# use model.evaluate to get the test loss and test accuracy\n",
        "test_loss, test_accuracy = model.evaluate(X_test.reshape(-1, 784), y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Task 9: Define a simple linear regression model using PyTorch\n",
        "# create a class LinearRegression that inherit from nn.Module\n",
        "# define the constructor and forward function\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # Assuming input and output features are 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Task 10: Train the linear regression model on some dummy data and print the weight and bias\n",
        "# create an instance of LinearRegression\n",
        "# use nn.MSELoss as criterion, optim.SGD as optimizer\n",
        "# use model.parameters() as input for optimizer\n",
        "# use optimizer.step() and criterion to update the model weight and bias\n",
        "\n",
        "model = LinearRegression()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Dummy data\n",
        "x_train = torch.tensor([[1.], [2.], [3.]])\n",
        "y_train = torch.tensor([[2.], [4.], [6.]])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):  # number of epochs\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x_train)\n",
        "    loss = criterion(output, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Print weight and bias\n",
        "print(f\"Weight: {model.linear.weight.item()}, Bias: {model.linear.bias.item()}\")\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goIaALYXVy1J"
      },
      "source": [
        "# Bonus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-ZYu5X7gV1L9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:18<00:00, 9121693.06it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.150\n",
            "[1,  4000] loss: 1.826\n",
            "[1,  6000] loss: 1.678\n",
            "[1,  8000] loss: 1.591\n",
            "[1, 10000] loss: 1.508\n",
            "[1, 12000] loss: 1.456\n",
            "[2,  2000] loss: 1.419\n",
            "[2,  4000] loss: 1.375\n",
            "[2,  6000] loss: 1.356\n",
            "[2,  8000] loss: 1.330\n",
            "[2, 10000] loss: 1.327\n",
            "[2, 12000] loss: 1.297\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Bonus Task: Implement a Convolutional Neural Network to classify the CIFAR-10 dataset\n",
        "# use torchvision.datasets.CIFAR10 to load the dataset\n",
        "# create a class CNN that inherit from nn.Module\n",
        "# define the constructor, forward function and the network architecture\n",
        "# use CrossEntropyLoss as criterion, optim.SGD as optimizer\n",
        "# use model.parameters() as input for optimizer\n",
        "# use optimizer.step() and criterion to update the model weight and bias\n",
        "\n",
        "# !pip install torchvision\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Transform the data to tensor and normalize it\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load the training and testing sets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)  # 3 input channels, 6 output channels, 5x5 kernel\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 pooling\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Fully connected layers\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)  # 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
